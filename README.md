<h3>Artificial Neural Network(ANN)</h3>

The term "Artificial Neural Network" is derived from Biological neural networks that develop the structure of a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural networks also have neurons that are interconnected to one another in various layers of the networks. These neurons are known as nodes.

<img src='https://static.javatpoint.com/tutorial/artificial-neural-network/images/artificial-neural-network2.png'>

<img src='https://static.javatpoint.com/tutorial/artificial-neural-network/images/artificial-neural-network4.png'>

<h4>Working of ANN</h4>

<img src='https://static.javatpoint.com/tutorial/artificial-neural-network/images/artificial-neural-network6.png'>

<h3>Ensemble Learning</h3>

In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.

<img src='https://miro.medium.com/max/1282/1*mRkymNb1AkcN-4Af4myHpQ.png'>

<h3>Bagging</h3>

Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.

<img src='https://miro.medium.com/max/1156/1*4uA23E47rv82m_myCkp6wQ.png'>

<img src='https://static.packt-cdn.com/products/9781788830577/graphics/ae3f74fc-6b16-4c24-8eb6-f90562052078.png'>

<h3>Boosting</h3>

The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.

<img src='https://cdn.educba.com/academy/wp-content/uploads/2019/11/bagging-and-boosting.png'>

<img src='https://quantdare.com/wp-content/uploads/2016/04/bb3-800x307.png'>

<h3>Naive Bayes Classifier</h3>

In statistics, Naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong independence assumptions between the features. They are among the simplest Bayesian network models, but coupled with Kernel density estimation, they can achieve higher accuracy levels.

<h4>Types:</h4>

There are three types of Naive Bayes model under the scikit-learn library:

<ul>
<li><b>Gaussian:</b> It is used in classification and it assumes that features follow a normal distribution.</li>
<li><b>Multinomial:</b> It is used for discrete counts. ...</li>
<li><b>Bernoulli:</b> The binomial model is useful if your feature vectors are binary (i.e. zeros and ones).</li>
</ul>
